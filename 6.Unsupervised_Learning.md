有监督学习（Supervised Learning）下的训练集：
$$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{2})$$

无监督学习（Unsupervised Learning）下的训练集：
$$(x^{(1)}), (x^{(2)}), (x^{(3)})$$

# K-means Clustering
## Interview questions:
1. How is k-means clustering different from hierarchical clustering?
K-means clustering specifically tries to put the data into the numbers of clusters you tell it to.
Hierarchical clustering just tells you, pairwise, what two things are most similar. 

## 算法步骤: 
1. 根据设定的聚类数$K$，随机地选择$K$个聚类中心（Cluster Centroid）
2. 评估各个样本到聚类中心的距离，如果样本距离第$i$个聚类中心更近，则认为其属于第$i$簇
3. 计算每个簇中样本的平均（Mean）位置，将聚类中心移动至该位置
重复以上步骤直至各个聚类中心的位置不再发生改变。

*note: 某些聚类中心可能没有被分配到样本，这样的聚类中心就会被淘汰（意味着最终的类数可能会减少）。*

## 伪码描述: 
* 假设簇的个数被定为$K$ ，样本数为$m$。
* 随机设定$K$个聚类中心：$\mu_1,\mu_2,...,\mu_k \in R^n$

重复如下过程直至聚类中心的位置不再改变：

**分配过程:**

for $i=1$ to $m$: 

$c^{(i)} =$距 $x^{(i)}$ 最近的聚类中心

距离的计算式如下：$\min_k||x^{(i)}-\mu_k||^2$


**移动过程:**

for $k = 1$ to $K$:

$\mu_k$（第$k$个聚类中心的新位置）= 第$k$簇的平均位置

假设$\mu_2$聚类中心下分配了4个样本：$x^{(1)},x^{(5)},x^{(6)},x^{(10)}$

亦即：$c^{(1)}=c^{(5)}=c^{(6)}=c^{(10)}=2$

那么$μ_2$将会移动到这四个样本的中心位置：$\mu_2=\frac{1}{4}(x^{(1)}+x^{(5)}+x^{(6)}+x^{(10)})$


## 优化(Optimization)
在引入K-Means的代价函数之前，先引入如下定义：

$\mu_c^{(i)}$ = 样本$x^{(i)}$被分配到的聚类中心

引入代价函数 - 失真代价函数(Distortion Cost Function)：
$J(c^{(1)},c^{(2)},...,c^{(m)};\mu_1,\mu_2,...,\mu_k)=\frac{1}{m}\sum_{i=1}^m||x^{(i)}-\mu_c(i)||^2$

## Random Initialization(如何初始化聚类中心)
通常，我们会随机选取$K$个样本作为$K$个聚类中心$(K<m)$。但是，如下图所示，不同的初始化有可能引起不同的聚类结果，能达到全局最优（global optimal）固然是好的，但是，往往得到的是局部最优（local optimal）。
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/attachments/%E8%81%9A%E7%B1%BB%E7%BB%93%E6%9E%9C1.png)
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/attachments/%E8%81%9A%E7%B1%BB%E7%BB%93%E6%9E%9C2.png)

for $i = 1$ to 100: 
1. 随机初始化，执行 K-Means，得到每个所属的簇$c^{(i)}$，以及各聚类的中心位置$\mu$：
$c^{(1)},c^{(2)},...,c^{(m)},\mu_1,\mu_2,...,\mu_k$
2. 计算失真函数$J$ 

选择100次中，J最小的作为最终的聚类结果。

*notes: 该方法计算量较大，所以只适用于$K$值较小的场景。*

## Choosing the Number of Clusters(如何确定聚类数)
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/attachments/%E8%82%98%E9%83%A8%E6%B3%95%E5%88%99.png)

“肘关节”部分对应的$K$值就是最恰当的$K$值，但是并不是所有代价函数曲线都存在明显的“肘关节”，例如下面的曲线：
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/attachments/%E6%B2%A1%E6%9C%89%E8%82%98.png)

一般来说，K-Means得到的聚类结果是服务于我们的后续目的（如通过聚类进行市场分析），所以不能脱离实际而单纯以数学方法来选择K值。在下面这个例子中，假定我们的衣服想要是分为S,M,L三个尺码，就设定K=3，如果我们想要XS、S、M、L、XL 5个衣服的尺码，就设定K=5：
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/attachments/%E6%9C%8D%E9%A5%B0%E5%A4%A7%E5%B0%8F.png)

### Bisecting K-means(二分K-Means算法)
常规的K-Means算法的误差通常只能收敛到局部最小，在此，引入一种称为二分K-Means（bisecting k-means）的算法，相较于常规的K-Means，二分K-Means不急于一来就随机K个聚类中心，而是首先把所有点归为一个簇，然后将该簇一分为二。计算各个所得簇的失真函数（即误差），选择误差最大的簇再进行划分（即最大程度地减少误差），重复该过程直至达到期望的簇数目。

二分K-Means算法流程大致如下：
1. 初始时，所有样本被看做在同一个簇：
   $c^{(1)}=c^{(2)}=...=c^{(m)}$
2. while num < k(num表示当前的簇数)：
   for i = 1 to num: 
* 计算误差
* 在该簇上进行K-Means聚类，其中k=2
* 计算将该簇一分为二后的总误差，该误差在之后将会被用于比较

选择使得总误差最小的簇进行划分。

虽然二分 K-Means 能带来全局最优解，但是我们也可以看到，该算法是一个贪心算法，因此计算量不小。

代码实现: [K-means]('https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/codes/KMeans.html)