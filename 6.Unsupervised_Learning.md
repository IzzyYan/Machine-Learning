有监督学习（Supervised Learning）下的训练集：
$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{2})$

无监督学习（Unsupervised Learning）下的训练集：
$(x^{(1)}), (x^{(2)}), (x^{(3)})$

# K-means Clustering
## Interview questions:
1. How is k-means clustering different from hierarchical clustering?
K-means clustering specifically tries to put the data into the numbers of clusters you tell it to.
Hierarchical clustering just tells you, pairwise, what two things are most similar. 

## 算法步骤: 
1. 根据设定的聚类数$K$，随机地选择$K$个聚类中心（Cluster Centroid）
2. 评估各个样本到聚类中心的距离，如果样本距离第$i$个聚类中心更近，则认为其属于第$i$簇
3. 计算每个簇中样本的平均（Mean）位置，将聚类中心移动至该位置
重复以上步骤直至各个聚类中心的位置不再发生改变。

*note: 某些聚类中心可能没有被分配到样本，这样的聚类中心就会被淘汰（意味着最终的类数可能会减少）。*

## 伪码描述: 
* 假设簇的个数被定为$K$ ，样本数为$m$。
* 随机设定$K$个聚类中心：$\mu_1,\mu_2,...,\mu_k \in R^n$

重复如下过程直至聚类中心的位置不再改变：

**分配过程:**

for $i=1$ to $m$: 

$c^{(i)} =$距 $x^{(i)}$ 最近的聚类中心

距离的计算式如下：$\min_k||x^{(i)}-\mu_k||^2$


**移动过程:**

for $k = 1$ to $K$:

$\mu_k$（第$k$个聚类中心的新位置）= 第$k$簇的平均位置

假设$\mu_2$聚类中心下分配了4个样本：$x^{(1)},x^{(5)},x^{(6)},x^{(10)}$

亦即：$c^{(1)}=c^{(5)}=c^{(6)}=c^{(10)}=2$

那么$μ_2$将会移动到这四个样本的中心位置：$\mu_2=\frac{1}{4}(x^{(1)}+x^{(5)}+x^{(6)}+x^{(10)})$


## 优化(Optimization)
在引入K-Means的代价函数之前，先引入如下定义：

$\mu_c^{(i)}$ = 样本$x^{(i)}$被分配到的聚类中心

引入代价函数 - 失真代价函数(Distortion Cost Function)：
$J(c^{(1)},c^{(2)},...,c^{(m)};\mu_1,\mu_2,...,\mu_k)=\frac{1}{m}\sum_{i=1}^m||x^{(i)}-\mu_c(i)||^2$

## Random Initialization(如何初始化聚类中心)
通常，我们会随机选取$K$个样本作为$K$个聚类中心$(K<m)$。但是，如下图所示，不同的初始化有可能引起不同的聚类结果，能达到全局最优（global optimal）固然是好的，但是，往往得到的是局部最优（local optimal）。

![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/attachments/%E8%81%9A%E7%B1%BB%E7%BB%93%E6%9E%9C1.png)
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/attachments/%E8%81%9A%E7%B1%BB%E7%BB%93%E6%9E%9C2.png)

for $i = 1$ to 100: 
1. 随机初始化，执行 K-Means，得到每个所属的簇$c^{(i)}$，以及各聚类的中心位置$\mu$：
$c^{(1)},c^{(2)},...,c^{(m)},\mu_1,\mu_2,...,\mu_k$
2. 计算失真函数$J$ 

选择100次中，J最小的作为最终的聚类结果。

*notes: 该方法计算量较大，所以只适用于$K$值较小的场景。*

## Choosing the Number of Clusters(如何确定聚类数)
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/attachments/%E8%82%98%E9%83%A8%E6%B3%95%E5%88%99.png)

“肘关节”部分对应的$K$值就是最恰当的$K$值，但是并不是所有代价函数曲线都存在明显的“肘关节”，例如下面的曲线：
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/attachments/%E6%B2%A1%E6%9C%89%E8%82%98.png)

一般来说，K-Means得到的聚类结果是服务于我们的后续目的（如通过聚类进行市场分析），所以不能脱离实际而单纯以数学方法来选择K值。在下面这个例子中，假定我们的衣服想要是分为S,M,L三个尺码，就设定K=3，如果我们想要XS、S、M、L、XL 5个衣服的尺码，就设定K=5：
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/attachments/%E6%9C%8D%E9%A5%B0%E5%A4%A7%E5%B0%8F.png)

### Bisecting K-means(二分K-Means算法)
常规的K-Means算法的误差通常只能收敛到局部最小，在此，引入一种称为二分K-Means（bisecting k-means）的算法，相较于常规的K-Means，二分K-Means不急于一来就随机K个聚类中心，而是首先把所有点归为一个簇，然后将该簇一分为二。计算各个所得簇的失真函数（即误差），选择误差最大的簇再进行划分（即最大程度地减少误差），重复该过程直至达到期望的簇数目。

二分K-Means算法流程大致如下：
1. 初始时，所有样本被看做在同一个簇：
   $c^{(1)}=c^{(2)}=...=c^{(m)}$
2. while num < k(num表示当前的簇数)：
   for i = 1 to num: 
* 计算误差
* 在该簇上进行K-Means聚类，其中k=2
* 计算将该簇一分为二后的总误差，该误差在之后将会被用于比较

选择使得总误差最小的簇进行划分。

虽然二分 K-Means 能带来全局最优解，但是我们也可以看到，该算法是一个贪心算法，因此计算量不小。

代码实现: [K-means](https://yoyoyohamapi.gitbooks.io/mit-ml/content/KMeans/codes/KMeans.html)


# Dimensionality Reduction
将高维特征投影到低维特征

## Principle Component Analysis(PCA)
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/attachments/PCA3D22D.png)

为了将特征维度从三维降低到二位，PCA就会先找寻两个三维向量$u^{(1)}, u^{(2)}$, 二者构成了一个二维平面，然后将原来的三维特征投影到该二维平面上

### 算法流程
假定我们需要将特征维度从n维降到k维。则PCA的执行流程如下：
1. 特征标准化，平衡各个特征尺度：
   
   $x_j^{(i)}=\frac{x_j^{(i)}-\mu_j}{s_j}$, $\mu_j$ 为特征 $j$ 的均值，$s_j$ 为特征 $j$ 的标准差。

2. 计算协方差矩阵$\Sigma$: 
   
   $\Sigma = \frac{1}{m}\sum\limits_{i=1}^{m}(x^{(i)})(x^{(i)})^T = \frac{1}{m} \cdot X^TX$

3. 通过奇异值分解（SVD），求取$\Sigma$的特征向量（eigenvectors）：
   
   $(U,S,V^T) = SVD(\Sigma)$

4. 从$U$中取出前k个左奇异向量，构成一个约减矩阵$U{reduce}$: 

   $U_{reduce} = (u^{(1)},u^{(2)},\cdots,u^{(k)})$

5. 计算新的特征向量：$z^{(i)}$: 

   $z^{(i)}=U_{reduce}^T \cdot x^{(i)}$

### 特征还原
因为PCA仅保留了特征的主成分，所以PCA是一种有损的压缩方式，假定我们获得新特征向量为：
$$z = U_{reduce}^Tx$$
则还原后的特征$x_{approx}$为：
$$x_{approx}=U_{reduce}z$$

### Choosing the number of principal components
从PCA的执行流程中，我们知道，需要为PCA指定目的维度k。如果降维不多，则性能提升不大；如果目标维度太小，则又丢失了许多信息。通常，使用如下的流程的来评估k值选取优异：
1. 求各样本的投影均方误差:
   
   $$\min\frac{1}{m}\sum\limits_{j=1}^{m}||x^{(i)}-x_{approx}^{(i)}||^2$$

2. 求数据的总变差：

   $$\frac{1}{m}\sum\limits_{j=1}^{m}||x^{(i)}||^2$$

3. 评估下式是否成立:

   $$\frac{\min\frac{1}{m}\sum\limits_{j=1}^{m}||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum\limits_{j=1}^{m}||x^{(i)}||^2} \leq \epsilon$$

其中，$\epsilon$ 的取值可以为0.01,0.05,0.10, 假设$\epsilon=0.01$，我们就说“特征间99%的差异性得到保留”。

### Andrew Ng的建议
PCA不是必须的，在机器学习中，一定谨记不要提前优化，只有当算法运行效率不尽如如人意时，再考虑使用PCA或者其他特征降维手段来提升训练速度。

#### 不只是加速学习
降低特征维度不只能加速模型的训练速度，还能帮我们在低维空间分析数据，例如，一个在三维空间完成的聚类问题，我们可以通过 PCA 将特征降低到二维平面进行可视化分析。

例如下表中，我们有将近几十个特征来描述国家的经济水平，但是你仔细观察发现，我们很难直观的看出各个国家的经济差异。
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/attachments/%E7%BB%8F%E6%B5%8E%E6%B0%B4%E5%B9%B3.png)

借助于 PCA，我们将特征降低到了二维，并在二维空间进行观察，很清楚的就能发现美国和新加坡具有很高的经济水平：
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/attachments/%E7%BB%8F%E6%B5%8E%E6%B0%B4%E5%B9%B3%E5%8F%AF%E8%A7%86%E5%8C%96.png)

推荐阅读: [强大的矩阵奇异值分解(SVD)及其应用](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)

[程序实例-PCA模型](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/codes/PCA%E6%A8%A1%E5%9E%8B.html)

[程序实例-PCA for 加速学习](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/codes/PCAfor%E5%8A%A0%E9%80%9F%E5%AD%A6%E4%B9%A0.html)
