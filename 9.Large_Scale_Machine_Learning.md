## 梯度下降
### 批量梯度下降法（Batch Gradient Descent）
拥有了大数据，就意味着，我们的算法模型中得面临一个很大的$m$值。回顾到我们的批量梯度下降法：

$$
\begin{align*}
&\mbox{重复直到收敛：} \\
& \quad \theta_j = \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)},
\quad \mbox{for $j=0,...,n$}
\end{align*}
$$
可以看到，每更新一个参数$θ_j$，我们都不得不遍历一遍样本集，在$m$很大时，该算法就显得比较低效。但是，批量梯度下降法能找到全局最优解：

<img src="https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/attachments/%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B.png" alt="avatar" style="zoom:50%;" />

### 随机梯度下降法（Stochastic Gradient Descent）

针对大数据集，又引入了随机梯度下降法，该算法的执行过程为：
$$
\begin{align*}
&\mbox{重复直到收敛：} \\
& \quad \mbox{for $i=1,...,m$：} \\
& \quad \quad \theta_j = \theta_j - \alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)},
\quad \mbox{for $j=0,...,n$}
\end{align*}
$$
相较于批量梯度下降法，随机梯度下降法每次更新 θjθj 只会用当前遍历的样本。虽然外层循环仍需要遍历所有样本，但是，往往我们能在样本尚未遍历完时就已经收敛，因此，面临大数据集时，随机梯度下降法性能卓越。

<img src="https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/attachments/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B.png" alt="avatar" style="zoom:50%;" />

上图反映了随机梯度下降法找寻最优解的过程，相较于批量梯度下降法，随机梯度下降法的曲线就显得不是那么平滑，而是很曲折了，其也倾向于找到局部最优解而不是全局最优解。因此，我们通常需要绘制调试曲线来监控随机梯度的工作过程是否正确。例如，假定误差定义为$cost(\theta, (x^{(i)}, y^{(i)})) = \frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$，则每完成 1000 次迭代，即遍历了 1000 个样本，我们求取平均误差并进行绘制，得到误差随迭代次数的变化曲线：

<img src="https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/attachments/%E8%AF%AF%E5%B7%AE%E5%8F%98%E5%8C%96%E6%9B%B2%E7%BA%BF1000.png" alt="avatar" style="zoom:50%;" />

另外，遇到下面的曲线也不用担心，其并不意味着我们的学习率出了问题，有可能是我们的平均间隔取的太小：

<img src="https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/attachments/%E8%AF%AF%E5%B7%AE%E5%8F%98%E5%8C%96%E6%9B%B2%E7%BA%BF%E6%8A%96%E5%8A%A8.png" alt="avatar" style="zoom:50%;" />

如果，我们每进行 5000 次迭代才进行绘制，那么曲线将更加平滑：

<img src="https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/attachments/%E8%AF%AF%E5%B7%AE%E5%8F%98%E5%8C%96%E6%9B%B2%E7%BA%BF5000.png" alt="avatar" style="zoom:50%;" />

如果我们面临明显上升态势的曲线，就要考虑降低学习率$α$了：

<img src="https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/attachments/%E8%AF%AF%E5%B7%AE%E5%8F%98%E5%8C%96%E6%9B%B2%E7%BA%BF%E4%B8%8A%E5%8D%87.png" alt="avatar" style="zoom:50%;" />

另外，学习率 αα 还可以随着迭代次数进行优化
$$
\alpha = \frac{constant1} {iterationNumber + constant2}
$$
这样，随着迭代次数的增多，我们的下降步调就会放缓，避免出现抖动：

<img src="https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/attachments/%E5%8A%A8%E6%80%81%E5%AD%A6%E4%B9%A0%E7%8E%87.png" alt="avatar" style="zoom:50%;" />

**note: 随机梯度下降法工作前，需要先乱序数据集，使得遍历样本的过程更加分散。**

### 小批量梯度下降法（Mini-batch Gradient Descent）

小批量梯度下降法是批量梯度下降法和随机梯度下降法的折中，通过参数$b$指明了每次迭代时，用于更新$θ$的样本数。假定$b=10,m=1000$，小批量梯度下降法的工作过程如下：
$$
\begin{align*}
&\mbox{重复直到收敛：} \\
& \quad \mbox{for $i=1,11,21,...,991$：} \\
& \quad \quad \theta_j = \theta_j - \alpha\frac{1}{10}\sum_{k=i}^{i+9}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)},
\quad \mbox{for $j=0,...,n$}
\end{align*}
$$

## Online Learning & Mapreduce

### Online Learning（在线学习）

用户登录了某提供货运服务的网站，输入了货运的发件地址和收件地址，该网站给出了货运报价，用户决定是购买该服务$(y=1)$或者是放弃购买该服务$(y=0)$。

特征向量$x$包括了收发地址，报价信息，我们想要学习$p(y=1|x;θ)$来最优化报价
$$
\begin{align*}
& \mbox{重复直到收敛：} \\
& \quad \mbox{获得关于该用户的样本 $(x,y)$，使用该样本更新 $\theta$：} \\
& \quad \quad \theta_j= \theta_j - \alpha(h_\theta(x)-y) x_j, \quad \mbox{for $j=0,..,n$}
\end{align*}
$$


与前面章节提到的机器学习过程不同，在线学习并不需要一个固定的样本集进行学习，而是不断接收样本，不断通过接收到的样本进行学习。因此，在线学习的前提是：我们面临着**流动的数据**。

### MapReduce

前面，我们提到了小批量梯度下降法，假定$b=400,m=400,000,000$，我们对$θ$的优化就为：
$$
\theta_j = \theta_j - \alpha\frac{1}{400}\sum{i=1}^{400}(h_\theta(x^{i})-y^{(i)})x_j^{(i)}
$$
假定我们有 4 个机器（Machine），我们首先通过 Map （映射）过程来并行计算式中的求和项，每个机器被分配到100个样本进行计算：
$$
temp_j^{(1)} =  \sum_{i=1}^{100}(h_\theta(x^{(i)} - y^{(i)})x_j^{(i)} \\
temp_j^{(2)} =  \sum_{i=101}^{200}(h_\theta(x^{(i)} - y^{(i)})x_j^{(i)} \\
temp_j^{(3)} =  \sum_{i=201}^{300}(h_\theta(x^{(i)} - y^{(i)})x_j^{(i)} \\
temp_j^{(4)} =  \sum_{i=301}^{400}(h_\theta(x^{(i)} - y^{(i)})x_j^{(i)}
$$
最后，通过Reduce（规约）操作进行求和：
$$
\theta_j = \theta_j - \alpha\frac{1}{400}(temp_j^{(1)}+temp_j^{(2)}+temp_j^{(3)}+temp_j^{(4)})
$$
我们可以使用多台机器进行MapReduce，此时，Map任务被分配到多个机器完成：

<img src="https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/attachments/%E5%A4%9A%E6%9C%BAMR.png" alt="avatar" style="zoom:50%;" />

也可以使用单机多核心进行MapReduce，此时，Map任务被分配到多个 CPU 核心完成：

<img src="https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/attachments/%E5%A4%9A%E6%A0%B8MR.png" alt="avatar" style="zoom:50%;" />

