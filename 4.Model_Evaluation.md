对于degree的确定，是我们对于模型的选择（Model Selection），正如我们在线性回归中确定$\theta$一样。在线性回归中，我们通过训练集确定模型，测试集来评估模型的泛化能力。在多项式回归中，我们通过训练集获得了参数$\theta$ ，而通过测试集确定了模型，那么，这两个集合用完了，我们就缺少评估模型泛化能力的数据集。鉴于此，引入了交叉验证集（Cross Validation Set），“交叉”二字体现了一种承上启下的关系，他通过训练集获得的结果，选择了模型，并将该模型交给测试集进行评估：
- 训练集：60%，确定参数$\theta$
- 交叉验证集：20%，进行模型选择。
- 测试集：20%，评价模型预测能力。

## 多项式回归中偏差与方差
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/attachments/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88.jpg)

![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/attachments/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%E7%9A%84%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE.png)

- 如果多项式次数较高，则容易造成过拟合，此时训练误差很低，但是对于新数据的泛化能力较差，导致交叉验证集和测试集的误差都很高，此时模型出现了高方差：
$$ J_{train}(\theta) is small$$

$$J_{cv}(\theta) >> J_{test}(\theta)$$

- 而当次数较低时，又易出现欠拟合的状况，此时无论是训练集，交叉验证集，还是测试集，都会有很高的误差，此时模型出现了高偏差：
$$ J_{train}(\theta) is large$$
$$ J_{cv}(\theta) \approx J_{train}(\theta)$$


## 正则化过程的偏差与方差
正则化（Regularization）能帮我们解决过拟合问题， λ取值越大，对参数θ的惩罚力度就越大，能够帮助解决过拟合问题，但是，如果惩罚过重，也会造成欠拟合问题，即会出现高偏差。如果λ取值较小，则意味着没有惩罚θ，也就不能解决过拟合问题，会出校高方差：
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/attachments/%E6%AD%A3%E8%A7%84%E5%8C%96%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88.jpg)

下图反映了正规化过程中，训练集、交叉验证集误差随λ变化的曲线：

![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/attachments/%E6%AD%A3%E8%A7%84%E5%8C%96%E7%9A%84%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE.png)


## 样本数目对与偏差方差的影响
当训练样本的数目m较小时，意味着可供学习的知识较少，则模型在训练阶段不容易犯错误（训练集误差极低），但也发现不了数据的规律（交叉验证集误差极高）；而当样本数目增多时，意味着需要学习的知识增多，则模型虽然在训练阶段容易犯一些错（训练集误差开始增高），但也更容易探测出数据规律（交叉验证集误差降低）：
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/attachments/%E6%A0%B7%E6%9C%AC%E6%95%B0%E7%9B%AE.jpg)

- 如果模型出现了高偏差，即出现了欠拟合，学习曲线随样本数目的变化曲线如下图所示，即增加样本数目，仍无法显著降低交叉验证集误差，即无法提高模型的泛化能力：
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/attachments/%E6%A0%B7%E6%9C%AC%E6%95%B0%E7%9B%AE%E9%AB%98%E5%81%8F%E5%B7%AE.jpg)

- 如果模型出现了高方差，即出现了过拟合，学习曲线随着样本数目变化的曲线如下图所示，即增加样本数目，可以显著降低交叉验证集的误差，提高模型的泛化能力：
![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/attachments/%E6%A0%B7%E6%9C%AC%E6%95%B0%E7%9B%AE%E9%AB%98%E6%96%B9%E5%B7%AE.png)

## 神经网络结构对偏差方差的影响
- 当神经网络的结构简单时，则易出现高偏差，computationally cheaper：

![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/attachments/%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

- 当神经网络的结构过于复杂时，则易出现高方差，computationally expensive，此时可以通过增大$\lambda$来解决：

![avatar](https://yoyoyohamapi.gitbooks.io/mit-ml/content/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/attachments/%E5%A4%8D%E6%9D%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

通过改变neural network的的layer和hidden units，不断增加，来看什么时候$J(\theta)$最小

# 总结
|手段|使用场景|
|---|---|
|Get more training examples采集更多的样本|fixed high variance|
|Try smaller sets of features降低特征维度|fixed high variance|
|Try getting additional features采集更多的特征|fixed high bias|
|Try adding polynomial features进行高次多项式回归($x_1^2,x_2^2,x_1,x_2$,etc)|fixed high bias|
|降低参数$\lambda$|fixed high bias|
|增大参数$\lambda$|fixed high variance|